{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 68978,
     "databundleVersionId": 7709659,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30665,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,ConcatDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "#from ax.service.ax_client import AxClient\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequence_file, pssm_files_path, amino_acid_to_ix, struct_to_ix, label_file=None, indices=None):\n",
    "        self.sequence_data = pd.read_csv(sequence_file)\n",
    "        if indices is not None:\n",
    "            self.sequence_data = self.sequence_data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        self.pssm_files_path = pssm_files_path\n",
    "        if label_file:\n",
    "            self.label_data = pd.read_csv(label_file)\n",
    "            if indices is not None:\n",
    "                self.label_data = self.label_data.iloc[indices].reset_index(drop=True)\n",
    "        else:\n",
    "            self.label_data = pd.DataFrame({'PDB_ID': self.sequence_data['PDB_ID'], 'LABEL': [''] * len(self.sequence_data)})\n",
    "        \n",
    "        self.amino_acid_to_ix = amino_acid_to_ix\n",
    "        self.struct_to_ix = struct_to_ix if label_file else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pdb_id = self.sequence_data.iloc[idx, 0]\n",
    "        sequence = self.sequence_data.iloc[idx, 1]\n",
    "        sequence_encoded = [self.amino_acid_to_ix[aa] for aa in sequence]\n",
    "        \n",
    "        label_sequence = self.label_data[self.label_data['PDB_ID'] == pdb_id].iloc[0, 1]\n",
    "        label_encoded = [self.struct_to_ix[label] for label in label_sequence] if self.struct_to_ix else []\n",
    "\n",
    "        pssm_file = os.path.join(self.pssm_files_path, f\"{pdb_id}_train.csv\")\n",
    "        pssm_data = pd.read_csv(pssm_file).iloc[:, 2:].to_numpy()\n",
    "\n",
    "        return {\n",
    "            'sequence': torch.tensor(sequence_encoded, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_encoded, dtype=torch.long) if label_encoded else torch.tensor([]),\n",
    "            'pssm': torch.tensor(pssm_data, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def protein_collate_fn(batch):\n",
    "    sequences, labels, pssms = zip(*[(sample['sequence'], sample['labels'], sample['pssm']) for sample in batch])\n",
    "    \n",
    "    # Pad sequences and labels\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    # Pad PSSM data (assuming all PSSM profiles have the same number of columns)\n",
    "    pssms_padded = pad_sequence(pssms, batch_first=True, padding_value=0)  # padding_value\n",
    "    \n",
    "    return {'sequence': sequences_padded,\n",
    "            'labels': labels_padded,\n",
    "            'pssm': pssms_padded}\n",
    "\n",
    "# ConvNet model definition\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_amino_acids=20, num_classes=3):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Conv1d(num_amino_acids, 64, kernel_size=5, padding=2), nn.ReLU(), nn.BatchNorm1d(64))\n",
    "        self.layer2 = nn.Sequential(nn.Conv1d(64, 128, kernel_size=5, padding=2), nn.ReLU(), nn.BatchNorm1d(128))\n",
    "        self.layer3 = nn.Sequential(nn.Conv1d(128, 256, kernel_size=5, padding=2), nn.ReLU(), nn.BatchNorm1d(256))\n",
    "        self.output_layer = nn.Conv1d(256, num_classes, kernel_size=1)\n",
    "\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # sequences[batch_size, sequence_length, 20] Three-dimensional tensor\n",
    "        #sequences_expanded = sequences.unsqueeze(2).repeat(1, 1, pssms.shape[2])\n",
    "\n",
    "        # sequences and pssms Merge in sequence dimension\n",
    "        #x = torch.cat((sequences_expanded, pssms), dim=1)\n",
    "\n",
    "        # Application network layer\n",
    "        x = x.permute(0, 2, 1)  # Adjust the dimensions to match the input requirements of the convolution layer\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        # Returns the output tensor\n",
    "        return out.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Define the amino acid and structure mappings\n",
    "amino_acid_to_ix = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20}\n",
    "struct_to_ix = {'H': 0, 'E': 1, 'C': 2}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sequence_file = '/kaggle/input/deep-learning-for-msc-202324/seqs_train.csv'\n",
    "label_file ='/kaggle/input/deep-learning-for-msc-202324/labels_train.csv'\n",
    "pssm_files_path ='/kaggle/input/deep-learning-for-msc-202324/train/'\n",
    "\n",
    "sequence_data = pd.read_csv(sequence_file)\n",
    "\n",
    "# 读取序列数据以获取数据集大小\n",
    "sequence_data = pd.read_csv(sequence_file)\n",
    "\n",
    "# 获取数据集的索引\n",
    "indices = range(len(sequence_data))\n",
    "\n",
    "# 分割索引\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建训练和验证的数据集实例\n",
    "train_dataset = ProteinDataset(sequence_file, pssm_files_path, amino_acid_to_ix, struct_to_ix, label_file, indices=train_indices)\n",
    "val_dataset = ProteinDataset(sequence_file, pssm_files_path, amino_acid_to_ix, struct_to_ix, label_file, indices=val_indices)\n",
    "# 创建训练 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=protein_collate_fn, pin_memory=True)\n",
    "# 创建验证 DataLoader\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=protein_collate_fn, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the dataset and data loader 没有分\n",
    "#protein_dataset = ProteinDataset(sequence_file, pssm_files_path, label_file, amino_acid_to_ix, struct_to_ix)\n",
    "#dataloader = DataLoader(protein_dataset, batch_size=4, shuffle=True, collate_fn=protein_collate_fn,pin_memory=True)\n",
    "\n",
    "\n",
    "model =  ConvNet().to(device)  # my model\n",
    "criterion = nn.CrossEntropyLoss()  # Cross entropy loss function for classification problems\n",
    "optimizer = optim.Adam(model.parameters())  # Adam optimizer\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001)  #set lr\n",
    "\n",
    "\n",
    "\n",
    "#初始化列表\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "\n",
    "\n",
    "def train_model(train_loader, val_loader):\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        # Training loop\n",
    "        for batch in train_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            pssms = batch['pssm'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pssms)\n",
    "            loss = criterion(outputs.reshape(-1, 3), labels.reshape(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.numel()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0\n",
    "        correct_val_predictions = 0\n",
    "        total_val_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                pssms = batch['pssm'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(pssms)\n",
    "                loss = criterion(outputs.reshape(-1, 3), labels.reshape(-1))\n",
    "\n",
    "                # Accumulate loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 2)\n",
    "                correct_val_predictions += (predicted == labels).sum().item()\n",
    "                total_val_predictions += labels.numel()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val_predictions / total_val_predictions\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ensure you have defined and instantiated your DataLoader here\n",
    "    train_model(train_loader,val_loader)\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'prediction_model_1.pth')\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-16T17:34:38.637421Z",
     "iopub.execute_input": "2024-03-16T17:34:38.637937Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch [1/10], Train Loss: 0.0929, Train Accuracy: 0.8608\nEpoch [1/10], Validation Loss: 0.0851, Validation Accuracy: 0.8703\nEpoch [2/10], Train Loss: 0.0803, Train Accuracy: 0.8770\nEpoch [2/10], Validation Loss: 0.0786, Validation Accuracy: 0.8788\nEpoch [3/10], Train Loss: 0.0762, Train Accuracy: 0.8821\nEpoch [3/10], Validation Loss: 0.0782, Validation Accuracy: 0.8791\nEpoch [4/10], Train Loss: 0.0741, Train Accuracy: 0.8850\nEpoch [4/10], Validation Loss: 0.0755, Validation Accuracy: 0.8837\nEpoch [5/10], Train Loss: 0.0730, Train Accuracy: 0.8868\nEpoch [5/10], Validation Loss: 0.0755, Validation Accuracy: 0.8832\nEpoch [6/10], Train Loss: 0.0704, Train Accuracy: 0.8906\nEpoch [6/10], Validation Loss: 0.0765, Validation Accuracy: 0.8812\nEpoch [7/10], Train Loss: 0.0702, Train Accuracy: 0.8911\nEpoch [7/10], Validation Loss: 0.0766, Validation Accuracy: 0.8828\nEpoch [8/10], Train Loss: 0.0682, Train Accuracy: 0.8936\nEpoch [8/10], Validation Loss: 0.0749, Validation Accuracy: 0.8852\nEpoch [9/10], Train Loss: 0.0669, Train Accuracy: 0.8960\nEpoch [9/10], Validation Loss: 0.0754, Validation Accuracy: 0.8842\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_losses, label='Training Loss')\n",
    "plt.title('Training Loss Per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制准确率曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_accuracies, label='Training Accuracy')\n",
    "plt.title('Training Accuracy Per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#same ProteinDataset with train ，no label\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequence_file, pssm_files_path, amino_acid_to_ix, struct_to_ix, label_file=None,):\n",
    "        self.sequence_data = pd.read_csv(sequence_file)\n",
    "        self.pssm_files_path = pssm_files_path\n",
    "        self.actual_lengths = [len(seq) for seq in self.sequence_data['SEQUENCE']]\n",
    "\n",
    "        if label_file:\n",
    "            self.label_data = pd.read_csv(label_file)\n",
    "        else:\n",
    "            # Create an empty DataFrame with only the PDB_ID so that it will work without the tag\n",
    "            self.label_data = pd.DataFrame({'PDB_ID': self.sequence_data['PDB_ID'], 'LABEL': [''] * len(self.sequence_data)})\n",
    "        \n",
    "        self.amino_acid_to_ix = amino_acid_to_ix\n",
    "        self.struct_to_ix = struct_to_ix if label_file else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pdb_id = self.sequence_data.iloc[idx, 0]\n",
    "        sequence = self.sequence_data.iloc[idx, 1]\n",
    "        sequence_encoded = [self.amino_acid_to_ix[aa] for aa in sequence]\n",
    "        \n",
    "        label_sequence = self.label_data[self.label_data['PDB_ID'] == pdb_id].iloc[0, 1]\n",
    "        label_encoded = [self.struct_to_ix[label] for label in label_sequence] if self.struct_to_ix else []\n",
    "\n",
    "        pssm_file = f\"{self.pssm_files_path}/{pdb_id}_test.csv\"\n",
    "        pssm_data = pd.read_csv(pssm_file).iloc[:, 2:].to_numpy()\n",
    "\n",
    "        return {\n",
    "            'sequence': torch.tensor(sequence_encoded, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_encoded, dtype=torch.long) if label_encoded else torch.tensor([]),\n",
    "            'pssm': torch.tensor(pssm_data, dtype=torch.float),\n",
    "            'pdb_id': pdb_id, \n",
    "            'length': self.actual_lengths[idx]\n",
    "        }\n",
    "\n",
    "def protein_collate_fn(batch):\n",
    "    sequences = pad_sequence([item['sequence'] for item in batch], batch_first=True)\n",
    "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=-1)\n",
    "    \n",
    "    if any(item['pssm'].nelement() != 0 for item in batch):\n",
    "        pssms = pad_sequence([item['pssm'] for item in batch], batch_first=True)\n",
    "    else:\n",
    "        pssms = None\n",
    "\n",
    "    pdb_ids = [item['pdb_id'] for item in batch]\n",
    "    lengths = torch.tensor([item['length'] for item in batch])\n",
    "\n",
    "    return {'sequence': sequences, 'labels': labels, 'pssm': pssms, 'pdb_id': pdb_ids, 'length': lengths}\n",
    "\n",
    "#Add code to the training loop to calculate validation losses\n",
    "val_losses = []\n",
    "\n",
    "# Define the amino acid and structure mappings\n",
    "amino_acid_to_ix = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20}\n",
    "struct_to_ix = {'H': 0, 'E': 1, 'C': 2}\n",
    "# Define the inverse mapping from index to structure label\n",
    "sec_struct_mapping_inv = {0: 'H', 1: 'E', 2: 'C'}\n",
    "\n",
    "test_sequence_file = '/kaggle/input/deep-learning-for-msc-202324/seqs_test.csv'\n",
    "test_pssm_files_path ='/kaggle/input/deep-learning-for-msc-202324/test'\n",
    "# Load the test data\n",
    "# Load the trained model\n",
    "model = ConvNet().to(device)\n",
    "model.load_state_dict(torch.load('prediction_model_1.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Load the test data\n",
    "test_dataset = ProteinDataset(sequence_file=test_sequence_file, \n",
    "                              pssm_files_path=test_pssm_files_path, \n",
    "                              amino_acid_to_ix=amino_acid_to_ix, \n",
    "                              struct_to_ix=struct_to_ix,  # Pass None if there are no labels\n",
    "                              label_file=None)  # No label file for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=protein_collate_fn, pin_memory=True)\n",
    "\n",
    "# Initialize a list to hold the formatted predictions\n",
    "submission = [['ID', 'STRUCTURE']]\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        sequences = batch['sequence'].to(device)\n",
    "        pssms = batch['pssm'].to(device) if batch['pssm'] is not None else None\n",
    "        pdb_ids = batch['pdb_id']\n",
    "        actual_lengths = batch['length']\n",
    "        # Show batch information\n",
    "        print(f\"Processing batch {batch_idx + 1}/{len(test_loader)}\")\n",
    "\n",
    "        if pssms is not None:\n",
    "            #  sequences Add a dimension to the last dimension\n",
    "            sequences_expanded = sequences.unsqueeze(-1)\n",
    "            input_data = torch.cat((sequences_expanded, pssms), dim=2)\n",
    "        else:\n",
    "            input_data = sequences\n",
    "\n",
    "        outputs = model(pssms)\n",
    "        _, predicted_labels = torch.max(outputs, dim=2)\n",
    "        \n",
    "    \n",
    "        # length of each test，Ensure final quantity\n",
    "        for i, pdb_id in enumerate(pdb_ids):\n",
    "            actual_length = sequences.shape[1]\n",
    "            actual_length = actual_lengths[i].item()\n",
    "            print(f\"Processing {pdb_id} with {actual_length} residues\")\n",
    "\n",
    "            for residue_index in range(actual_length):\n",
    "                residue_prediction = predicted_labels[i][residue_index].cpu().item()\n",
    "                submission.append([f\"{pdb_id}_{residue_index + 1}\", sec_struct_mapping_inv[residue_prediction]])\n",
    "\n",
    "# Save the prediction to a file\n",
    "with open('predictions.csv', 'w') as f:\n",
    "    for line in submission:\n",
    "        f.write(','.join(line) + '\\n')\n",
    "\n",
    "print(f\"Total predictions made: {len(submission) - 1}\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
