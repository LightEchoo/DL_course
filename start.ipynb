{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 68978,
     "databundleVersionId": 7709659,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30665,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.service.utils.instantiation import ObjectiveProperties\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from ax.utils.notebook.plotting import render"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:09:49.408090Z",
     "start_time": "2024-03-18T12:09:42.350246Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install ax-platform"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# # 1. Observe the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# one_train_pssm = pd.read_csv('deep-learning-for-msc-202324/train/1A0A_3_A_train.csv')\n",
    "# one_train_pssm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:10:17.403861Z",
     "start_time": "2024-03-18T12:10:17.390857Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# seqs_test = pd.read_csv('deep-learning-for-msc-202324/seqs_test.csv')\n",
    "# seqs_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# seqs_train = pd.read_csv('deep-learning-for-msc-202324/seqs_train.csv')\n",
    "# seqs_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# labels_train = pd.read_csv('deep-learning-for-msc-202324/labels_train.csv')\n",
    "# labels_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    It is used to load and preprocess the protein sequence and label data. It also reads the PSSM data from the provided files.\n",
    "    \n",
    "    :param seq_file_path: The file path to the sequence data\n",
    "    :type seq_file_path: str\n",
    "    :param pssm_files_path: The directory path to the PSSM files\n",
    "    :type pssm_files_path: str\n",
    "    :param label_file_path: The file path to the label data\n",
    "    :type label_file_path: str\n",
    "    :param indices: The indices of the data to select\n",
    "    :type indices: list\n",
    "    \n",
    "    :return: The protein dataset\n",
    "    :rtype: Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_file_path, pssm_files_path, label_file_path, indices):\n",
    "        # Define the amino acid and structure mappings\n",
    "        self.amino_acid_to_ix = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10,\n",
    "                                 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19,\n",
    "                                 'Y': 20}\n",
    "        self.struct_to_ix = {'H': 0, 'E': 1, 'C': 2}\n",
    "\n",
    "        # Read the sequence data, and select a subset of the data if indices are provided(train set and val set)\n",
    "        self.seq_data = pd.read_csv(seq_file_path)\n",
    "        self.seq_data = self.seq_data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        # Read the PSSM files path\n",
    "        self.pssm_files_path = pssm_files_path\n",
    "\n",
    "        # Read the label data, and select a subset of the data if indices are provided\n",
    "        self.label_data = pd.read_csv(label_file_path)\n",
    "        self.label_data = self.label_data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        It is used to get the sequence, label, and PSSM data for a given index.\n",
    "        \n",
    "        :param idx: The index of the data to retrieve \n",
    "        :type idx: int\n",
    "        :return: The sequence, label, and PSSM data\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        pdb_id = self.seq_data.iloc[idx, 0]\n",
    "        sequence = self.seq_data.iloc[idx, 1]\n",
    "        sequence_encoded = [self.amino_acid_to_ix[aa] for aa in sequence]\n",
    "\n",
    "        label_sequence = self.label_data[self.label_data['PDB_ID'] == pdb_id].iloc[0, 1]\n",
    "        label_encoded = [self.struct_to_ix[label] for label in label_sequence]\n",
    "\n",
    "        pssm_file_path = os.path.join(self.pssm_files_path, f\"{pdb_id}_train.csv\")\n",
    "        pssm_data = pd.read_csv(pssm_file_path).iloc[:, 2:].to_numpy()\n",
    "\n",
    "        return {\n",
    "            'sequence': torch.tensor(sequence_encoded, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_encoded, dtype=torch.long),\n",
    "            'pssm': torch.tensor(pssm_data, dtype=torch.float),\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def protein_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    It is used to process and prepare batched data during data loading. The DataLoader object can accept a function through the collate_fn parameter to customize how to combine multiple samples into a batch. This is particularly useful when dealing with sequence data, as the lengths of sequence data are often not consistent and need to be padded or otherwise processed to ensure that all data in a batch have consistent dimensions.\n",
    "    :param batch: The batch of data to process\n",
    "    :type batch: int\n",
    "    :return: A dictionary containing the processed batched data\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    sequences, labels, pssms = zip(*[(sample['sequence'], sample['labels'], sample['pssm']) for sample in batch])\n",
    "\n",
    "    # Pad sequences and labels\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    # Pad PSSM data (assuming all PSSM profiles have the same number of columns)\n",
    "    pssms_padded = pad_sequence(pssms, batch_first=True, padding_value=-1)  # Use -1 to pad the PSSM data\n",
    "\n",
    "    return {'sequence': sequences_padded,\n",
    "            'labels': labels_padded,\n",
    "            'pssm': pssms_padded}"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ProteinSecondaryStructureFCNwithEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    It is used to define a fully convolutional neural network (FCN) model for protein secondary structure prediction. The model uses an embedding layer to embed the amino acid sequence data, and then concatenates the embedded sequence data with the PSSM data before applying convolutional layers to learn features from the data. The model outputs a sequence of predictions for the secondary structure of the protein.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=10, dropout_rate=0.5):\n",
    "        num_amino_acids = 20\n",
    "        num_pssm_features = 20\n",
    "        num_classes = 3\n",
    "        super(ProteinSecondaryStructureFCNwithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_amino_acids + 1,\n",
    "                                      embedding_dim=embedding_dim)\n",
    "\n",
    "        # convolutional layers\n",
    "        self.layer1 = nn.Sequential(nn.Conv1d(embedding_dim + num_pssm_features, 64, kernel_size=5, padding=2),\n",
    "                                    nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(dropout_rate))\n",
    "        self.layer2 = nn.Sequential(nn.Conv1d(64, 128, kernel_size=5, padding=2), nn.ReLU(), nn.BatchNorm1d(128),\n",
    "                                    nn.Dropout(dropout_rate))\n",
    "        self.layer3 = nn.Sequential(nn.Conv1d(128, 256, kernel_size=5, padding=2), nn.ReLU(), nn.BatchNorm1d(256),\n",
    "                                    nn.Dropout(dropout_rate))\n",
    "        self.output_layer = nn.Conv1d(256, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, sequence, pssm):\n",
    "        # sequence: [batch_size, seq_len]\n",
    "        # pssm: [batch_size, seq_len, num_pssm_features]\n",
    "\n",
    "        # embed the sequence\n",
    "        embedded_sequence = self.embedding(sequence)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # adjust the dimensions of the embedded sequence to be concatenated with the PSSM\n",
    "        embedded_sequence = embedded_sequence.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]\n",
    "\n",
    "        # concatenate the embedded sequence and PSSM data\n",
    "        combined_input = torch.cat((embedded_sequence, pssm.permute(0, 2, 1)),\n",
    "                                   dim=1)  # [batch_size, embedding_dim+num_pssm_features, seq_len]\n",
    "\n",
    "        # apply the network layers\n",
    "        out = self.layer1(combined_input)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        return out.permute(0, 2, 1)  # [batch_size, seq_len, num_classes]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size):\n",
    "    \"\"\"\n",
    "    It is used to create training and validation data loaders for the protein secondary structure prediction model. The function loads the sequence data, label data, and PSSM data, and then creates a training and validation split of the data. The function then creates data loaders for the training and validation data.\n",
    "    :param batch_size: batch size for the data loaders\n",
    "    :type batch_size: int\n",
    "    :return: a tuple containing the training and validation data loaders\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    seqs_file_path = 'kaggle/input/deep-learning-for-msc-202324/seqs_train.csv'\n",
    "    label_file_path = 'kaggle/input/deep-learning-for-msc-202324/labels_train.csv'\n",
    "    pssm_files_path = 'kaggle/input/deep-learning-for-msc-202324/train/'\n",
    "\n",
    "    # load the sequence data and generate a list of PDB IDs\n",
    "    seqs_data = pd.read_csv(seqs_file_path)\n",
    "    # pdb_ids = seqs_data['PDB_ID'].tolist()\n",
    "    indices = range(len(seqs_data))\n",
    "\n",
    "    # split the PDB_ID list into training and validation sets\n",
    "    pdb_ids_train, pdb_ids_val = train_test_split(indices, test_size=0.2, random_state=10)\n",
    "\n",
    "    # create training and validation data loaders\n",
    "    dataset_train = ProteinDataset(seqs_file_path, pssm_files_path, label_file_path, pdb_ids_train)\n",
    "    dataset_val = ProteinDataset(seqs_file_path, pssm_files_path, label_file_path, pdb_ids_val)\n",
    "\n",
    "    # create data loaders for training and validation\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=protein_collate_fn,\n",
    "                                  pin_memory=True)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, collate_fn=protein_collate_fn,\n",
    "                                pin_memory=True)\n",
    "\n",
    "    return dataloader_train, dataloader_val"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(batch_size=4, num_epochs=10, embedding_dim=10, dropout_rate=0.5, lr=0.0001):\n",
    "    \"\"\"\n",
    "    It is used to train the protein secondary structure prediction model. The function creates the data loaders, initializes the model, loss function, and optimizer, and then trains the model for the specified number of epochs. The function returns the trained model and the training and validation performance metrics.\n",
    "    :param batch_size: batch size for the data loaders\n",
    "    :type batch_size: int\n",
    "    :param num_epochs: the number of epochs to train the model\n",
    "    :type num_epochs: int\n",
    "    :param embedding_dim: the dimension of the embedding layer\n",
    "    :type embedding_dim: int\n",
    "    :param dropout_rate: the dropout rate for the model\n",
    "    :type dropout_rate: float\n",
    "    :param lr: the learning rate for the optimizer\n",
    "    :type lr: float\n",
    "    :return: a dictionary containing the trained model and the training and validation performance metrics\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    train_loader, val_loader = get_data_loaders(batch_size)\n",
    "    model = ProteinSecondaryStructureFCNwithEmbedding(embedding_dim=embedding_dim, dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Cross entropy loss function for classification problems\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam optimizer\n",
    "\n",
    "    # initialize lists to store performance metrics\n",
    "    metrics = {\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'train_precision': [],\n",
    "        'train_recall': [],\n",
    "        'train_f1': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        train_losses, train_true, train_pred = [], [], []\n",
    "\n",
    "        # Training loop\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            pssms = batch['pssm'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences, pssms)\n",
    "            loss = criterion(outputs.reshape(-1, 3), labels.reshape(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the training loss\n",
    "            train_losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            train_true.extend(labels.view(-1).cpu().numpy())\n",
    "            train_pred.extend(predicted.view(-1).cpu().numpy())\n",
    "\n",
    "        # Calculate the training performance metrics\n",
    "        metrics['train_loss'].append(np.mean(train_losses))\n",
    "        metrics['train_accuracy'].append(accuracy_score(train_true, train_pred))\n",
    "        metrics['train_precision'].append(precision_score(train_true, train_pred, average='macro', zero_division=0))\n",
    "        metrics['train_recall'].append(recall_score(train_true, train_pred, average='macro', zero_division=0))\n",
    "        metrics['train_f1'].append(f1_score(train_true, train_pred, average='macro', zero_division=0))\n",
    "\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {metrics[\"train_loss\"][-1]:.4f}, Train Accuracy: {metrics[\"train_accuracy\"][-1]:.4f}, Train Precision: {metrics[\"train_precision\"][-1]:.4f}, Train Recall: {metrics[\"train_recall\"][-1]:.4f}, Train F1: {metrics[\"train_f1\"][-1]:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_losses, val_true, val_pred = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                pssms = batch['pssm'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(sequences, pssms)\n",
    "                loss = criterion(outputs.reshape(-1, 3), labels.reshape(-1))\n",
    "\n",
    "                # Track the validation loss\n",
    "                val_losses.append(loss.item())\n",
    "                _, predicted = torch.max(outputs, 2)\n",
    "                val_true.extend(labels.view(-1).cpu().numpy())\n",
    "                val_pred.extend(predicted.view(-1).cpu().numpy())\n",
    "\n",
    "        # Calculate the validation performance metrics\n",
    "        metrics['val_loss'].append(np.mean(val_losses))\n",
    "        metrics['val_accuracy'].append(accuracy_score(val_true, val_pred))\n",
    "        metrics['val_precision'].append(precision_score(val_true, val_pred, average='macro', zero_division=0))\n",
    "        metrics['val_recall'].append(recall_score(val_true, val_pred, average='macro', zero_division=0))\n",
    "        metrics['val_f1'].append(f1_score(val_true, val_pred, average='macro', zero_division=0))\n",
    "\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {metrics[\"val_loss\"][-1]:.4f}, Validation Accuracy: {metrics[\"val_accuracy\"][-1]:.4f}, Validation Precision: {metrics[\"val_precision\"][-1]:.4f}, Validation Recall: {metrics[\"val_recall\"][-1]:.4f}, Validation F1: {metrics[\"val_f1\"][-1]:.4f}')\n",
    "\n",
    "    return {'model': model, 'metrics': metrics}"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Ensure you have defined and instantiated your DataLoader here\n",
    "# model, metrics = train_model()\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), 'prediction_model_1.pth')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-16T17:34:38.637421Z",
     "iopub.execute_input": "2024-03-16T17:34:38.637937Z"
    },
    "trusted": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_loss_accuracy_history(train_loss_history, train_accuracy_history, val_loss_history, val_accuracy_history):\n",
    "    \"\"\"\n",
    "    绘制训练和验证损失及准确率的趋势图。\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_loss_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss_history, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss_history, label='Validation Loss')\n",
    "    plt.title('Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracy_history, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracy_history, label='Validation Accuracy')\n",
    "    plt.title('Accuracy History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# plot_loss_accuracy_history(epoch_train_losses, epoch_train_accuracies, epoch_val_losses, epoch_val_accuracies)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_evaluate(parameterization):\n",
    "    \"\"\"\n",
    "    It is used to train and evaluate the protein secondary structure prediction model with the given hyperparameters.\n",
    "    :param parameterization: A dictionary containing the hyperparameters to use for training and evaluation\n",
    "    :type parameterization: dict\n",
    "    :return: A dictionary containing the validation performance metrics\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # extract the hyperparameters from the parameterization\n",
    "    learning_rate = parameterization[\"lr\"]\n",
    "    dropout_rate = parameterization[\"dropout_rate\"]\n",
    "    batch_size = parameterization[\"batch_size\"]\n",
    "    embedding_dim = parameterization[\"embedding_dim\"]\n",
    "\n",
    "    # use the extracted hyperparameters to train and evaluate the model\n",
    "    metrics = \\\n",
    "        train_model(batch_size=batch_size, embedding_dim=embedding_dim, dropout_rate=dropout_rate, lr=learning_rate)[\n",
    "            'metrics']\n",
    "\n",
    "    val_loss = metrics['val_loss'][-1]\n",
    "    val_accuracy = metrics['val_accuracy'][-1]\n",
    "    val_precision = metrics['val_precision'][-1]\n",
    "    val_recall = metrics['val_recall'][-1]\n",
    "    val_f1 = metrics['val_f1'][-1]\n",
    "\n",
    "    return {\"val_loss\": (val_loss, 0.0), \"val_accuracy\": (val_accuracy, 0.0), \"val_precision\": (val_precision, 0.0),\n",
    "            \"val_recall\": (val_recall, 0.0), \"val_f1\": (val_f1, 0.0)}"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip show ax-platform"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ax_client = AxClient()\n",
    "ax_client.create_experiment(\n",
    "    name=\"protein_structure_prediction_experiment\",\n",
    "    parameters=[\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 1e-2], \"log_scale\": True},\n",
    "        {\"name\": \"dropout_rate\", \"type\": \"range\", \"bounds\": [0.0, 0.2]},\n",
    "        {\"name\": \"batch_size\", \"type\": \"choice\", \"values\": [32,64,128]},\n",
    "        {\"name\": \"embedding_dim\", \"type\": \"range\", \"values\": [1, 10]},\n",
    "    ],\n",
    "    objectives={\"val_loss\": ObjectiveProperties(minimize=True)},\n",
    "    tracking_metric_names=[\"val_accuracy\", \"val_precision\", \"val_recall\", \"val_f1\"],  # 同时跟踪准确率\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "lr  0.000143\n",
    "droupout_rate 0.234673\n",
    "batch_size 32\n",
    "embedding_dim 15\n",
    "较好参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Validation Accuracy: 0.9202\n",
    "[INFO 03-17 15:58:31] ax.service.ax_client: Completed trial 1 with data: {'val_loss': (0.20919, 0.0), 'val_accuracy': (0.920199, 0.0), 'val_precision': (0.845093, 0.0), 'val_recall': (0.748058, 0.0), 'val_f1': (0.774647, 0.0)}."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "times_ax = 10\n",
    "for i in range(times_ax):  # 迭代次数\n",
    "    print(f\"Running optimization iteration {i+1}/{times_ax}...\")\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=train_evaluate(parameters))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_parameters = ax_client.get_best_parameters()\n",
    "display(best_parameters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "display(best_parameters[1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df = ax_client.get_trials_data_frame()\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df.to_csv('10_ax_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# times_ax = 2\n",
    "# for i in range(times_ax):\n",
    "#     print(f\"Running optimization iteration {i + 1}/{times_ax}...\")\n",
    "#     parameters, trial_index = ax_client.get_next_trial()\n",
    "#     ax_client.complete_trial(trial_index=trial_index, raw_data=train_evaluate(parameters))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# best_parameters = ax_client.get_best_parameters()\n",
    "# display(best_parameters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_parameters[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_parameters = best_parameters[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df = ax_client.get_trials_data_frame()\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df.to_csv('50_ax_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ax.utils.notebook.plotting import render"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "render(ax_client.get_contour_plot(param_x=\"lr\", param_y=\"dropout_rate\", metric_name=\"val_accuracy\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "render(ax_client.get_optimization_trace())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
