{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 68978,
     "databundleVersionId": 7709659,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30665,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.service.utils.instantiation import ObjectiveProperties\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from ax.utils.notebook.plotting import render\n",
    "from captum.attr import IntegratedGradients"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.368542Z",
     "start_time": "2024-03-18T12:29:26.151376Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=12.1 -c pytorch -c nvidia"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.383849Z",
     "start_time": "2024-03-18T12:29:32.369541Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install ax-platform"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.399852Z",
     "start_time": "2024-03-18T12:29:32.384850Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.447921Z",
     "start_time": "2024-03-18T12:29:32.400850Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# # 1. Observe the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# one_train_pssm = pd.read_csv('deep-learning-for-msc-202324/train/1A0A_3_A_train.csv')\n",
    "# one_train_pssm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.463679Z",
     "start_time": "2024-03-18T12:29:32.449925Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# seqs_test = pd.read_csv('deep-learning-for-msc-202324/seqs_test.csv')\n",
    "# seqs_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.478842Z",
     "start_time": "2024-03-18T12:29:32.466769Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# seqs_train = pd.read_csv('deep-learning-for-msc-202324/seqs_train.csv')\n",
    "# seqs_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.494026Z",
     "start_time": "2024-03-18T12:29:32.479843Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# labels_train = pd.read_csv('deep-learning-for-msc-202324/labels_train.csv')\n",
    "# labels_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.510022Z",
     "start_time": "2024-03-18T12:29:32.496025Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Define the dataset and data loaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    It is used to load and preprocess the protein sequence and label data. It also reads the PSSM data from the provided files.\n",
    "    \n",
    "    :param seq_file_path: The file path to the sequence data\n",
    "    :type seq_file_path: str\n",
    "    :param pssm_files_path: The directory path to the PSSM files\n",
    "    :type pssm_files_path: str\n",
    "    :param label_file_path: The file path to the label data\n",
    "    :type label_file_path: str\n",
    "    :param indices: The indices of the data to select\n",
    "    :type indices: list\n",
    "    \n",
    "    :return: The protein dataset\n",
    "    :rtype: Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_file_path, pssm_files_path, label_file_path=None, indices=None):\n",
    "        # Define the amino acid and structure mappings\n",
    "        self.amino_acid_to_ix = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10,\n",
    "                                 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19,\n",
    "                                 'Y': 20}\n",
    "        self.struct_to_ix = {'H': 0, 'E': 1, 'C': 2}\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # Read the sequence data, and select a subset of the data if indices are provided(train set and val set)\n",
    "        self.seq_data = pd.read_csv(seq_file_path)\n",
    "        self.real_lan = [len(seq) for seq in self.seq_data['SEQUENCE']]\n",
    "        if indices is not None:\n",
    "            self.seq_data = self.seq_data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "        # Read the PSSM files path\n",
    "        self.pssm_files_path = pssm_files_path\n",
    "\n",
    "        # Read the label data, and select a subset of the data if indices are provided\n",
    "        if label_file_path:\n",
    "            self.label_data = pd.read_csv(label_file_path)\n",
    "            if indices is not None:\n",
    "                self.label_data = self.label_data.iloc[indices].reset_index(drop=True)\n",
    "        else:\n",
    "            self.label_data = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        It is used to get the sequence, label, and PSSM data for a given index.\n",
    "        \n",
    "        :param idx: The index of the data to retrieve \n",
    "        :type idx: int\n",
    "        :return: The sequence, label, and PSSM data\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        pdb_id = self.seq_data.iloc[idx, 0]\n",
    "        sequence = self.seq_data.iloc[idx, 1]\n",
    "        sequence_encoded = [self.amino_acid_to_ix[aa] for aa in sequence]\n",
    "\n",
    "        if self.label_data is not None:\n",
    "            label_sequence = self.label_data[self.label_data['PDB_ID'] == pdb_id].iloc[0, 1]\n",
    "            label_encoded = [self.struct_to_ix[label] for label in label_sequence]\n",
    "        else:\n",
    "            label_encoded = [-1] * len(sequence)  # 使用 -1 作为未知标签的占位符\n",
    "\n",
    "        pssm_file_path = os.path.join(self.pssm_files_path, f\"{pdb_id}_test.csv\" if self.label_data is None else f\"{pdb_id}_train.csv\")\n",
    "        pssm_data = pd.read_csv(pssm_file_path).iloc[:, 2:].to_numpy()\n",
    "\n",
    "        return {\n",
    "            'pdb_id': pdb_id,\n",
    "            'sequence': torch.tensor(sequence_encoded, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_encoded, dtype=torch.long),\n",
    "            'pssm': torch.tensor(pssm_data, dtype=torch.float),\n",
    "            'real_len': self.real_lan[idx]\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.525180Z",
     "start_time": "2024-03-18T12:29:32.511024Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def protein_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    It is used to process and prepare batched data during data loading. The DataLoader object can accept a function through the collate_fn parameter to customize how to combine multiple samples into a batch. This is particularly useful when dealing with sequence data, as the lengths of sequence data are often not consistent and need to be padded or otherwise processed to ensure that all data in a batch have consistent dimensions.\n",
    "    :param batch: The batch of data to process\n",
    "    :type batch: int\n",
    "    :return: A dictionary containing the processed batched data\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    sequences, labels, pssms, pdb_ids, real_lens = zip(*[(sample['sequence'], sample['labels'], sample['pssm'], sample['pdb_id'], sample['real_len']) for sample in batch])\n",
    "\n",
    "\n",
    "    # Pad sequence and label data\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    # Pad PSSM data\n",
    "    pssms_padded = pad_sequence(pssms, batch_first=True, padding_value=-1)  # Use -1 to pad the PSSM data\n",
    "\n",
    "    return {'sequence': sequences_padded,\n",
    "            'labels': labels_padded,\n",
    "            'pssm': pssms_padded,\n",
    "            'pdb_id': pdb_ids,\n",
    "            'real_len': torch.tensor(real_lens)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.541182Z",
     "start_time": "2024-03-18T12:29:32.527173Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size):\n",
    "    \"\"\"\n",
    "    It is used to create training and validation data loaders for the protein secondary structure prediction model. The function loads the sequence data, label data, and PSSM data, and then creates a training and validation split of the data. The function then creates data loaders for the training and validation data.\n",
    "    :param batch_size: batch size for the data loaders\n",
    "    :type batch_size: int\n",
    "    :return: a tuple containing the training and validation data loaders\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    seqs_file_path = 'kaggle/input/deep-learning-for-msc-202324/seqs_train.csv'\n",
    "    label_file_path = 'kaggle/input/deep-learning-for-msc-202324/labels_train.csv'\n",
    "    pssm_files_path = 'kaggle/input/deep-learning-for-msc-202324/train/'\n",
    "\n",
    "    # load the sequence data and generate a list of PDB IDs\n",
    "    seqs_data = pd.read_csv(seqs_file_path)\n",
    "    # pdb_ids = seqs_data['PDB_ID'].tolist()\n",
    "    indices = range(len(seqs_data))\n",
    "\n",
    "    # split the PDB_ID list into training and validation sets\n",
    "    pdb_ids_train, pdb_ids_val = train_test_split(indices, test_size=0.2, random_state=10)\n",
    "\n",
    "    # create training and validation data loaders\n",
    "    dataset_train = ProteinDataset(seqs_file_path, pssm_files_path, label_file_path, pdb_ids_train)\n",
    "    dataset_val = ProteinDataset(seqs_file_path, pssm_files_path, label_file_path, pdb_ids_val)\n",
    "\n",
    "    # create data loaders for training and validation\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=protein_collate_fn,\n",
    "                                  pin_memory=True)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, collate_fn=protein_collate_fn,\n",
    "                                pin_memory=True)\n",
    "\n",
    "    return dataloader_train, dataloader_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.556440Z",
     "start_time": "2024-03-18T12:29:32.542784Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Design the model(Fully Convolutional Networks, FCN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ProteinSecondaryStructureFCNwithEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    It is used to define a fully convolutional neural network (FCN) model for protein secondary structure prediction. The model uses an embedding layer to embed the amino acid sequence data, and then concatenates the embedded sequence data with the PSSM data before applying convolutional layers to learn features from the data. The model outputs a sequence of predictions for the secondary structure of the protein.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=10, dropout_rate=0.5):\n",
    "        num_amino_acids = 20\n",
    "        num_pssm_features = 20\n",
    "        num_classes = 3\n",
    "        super(ProteinSecondaryStructureFCNwithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_amino_acids + 1,\n",
    "                                      embedding_dim=embedding_dim)\n",
    "\n",
    "        # convolutional layers\n",
    "        self.layer1 = nn.Sequential(nn.Conv1d(embedding_dim + num_pssm_features, 64, kernel_size=5, padding=2),\n",
    "                                    nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(dropout_rate))\n",
    "        self.layer2 = nn.Sequential(nn.Conv1d(64, 128, kernel_size=5, padding=2), nn.ReLU(), nn.BatchNorm1d(128),\n",
    "                                    nn.Dropout(dropout_rate))\n",
    "        self.layer3 = nn.Sequential(nn.Conv1d(128, 256, kernel_size=5, padding=2), nn.ReLU(), nn.BatchNorm1d(256),\n",
    "                                    nn.Dropout(dropout_rate))\n",
    "        self.layer4 = nn.Sequential(nn.Conv1d(256, 512, kernel_size=5, padding=2), nn.ReLU(), nn.BatchNorm1d(512),\n",
    "                                    nn.Dropout(dropout_rate))\n",
    "        self.output_layer = nn.Conv1d(512, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, sequence, pssm):\n",
    "        # sequence: [batch_size, seq_len]\n",
    "        # pssm: [batch_size, seq_len, num_pssm_features]\n",
    "\n",
    "        # embed the sequence\n",
    "        embedded_sequence = self.embedding(sequence)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # adjust the dimensions of the embedded sequence to be concatenated with the PSSM\n",
    "        embedded_sequence = embedded_sequence.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]\n",
    "\n",
    "        # concatenate the embedded sequence and PSSM data\n",
    "        combined_input = torch.cat((embedded_sequence, pssm.permute(0, 2, 1)),\n",
    "                                   dim=1)  # [batch_size, embedding_dim+num_pssm_features, seq_len]\n",
    "\n",
    "        # apply the network layers\n",
    "        out = self.layer1(combined_input)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        return out.permute(0, 2, 1)  # [batch_size, seq_len, num_classes]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.572445Z",
     "start_time": "2024-03-18T12:29:32.558447Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(batch_size=4, num_epochs=10, embedding_dim=10, dropout_rate=0.5, lr=0.0001):\n",
    "    \"\"\"\n",
    "    It is used to train the protein secondary structure prediction model. The function creates the data loaders, initializes the model, loss function, and optimizer, and then trains the model for the specified number of epochs. The function returns the trained model and the training and validation performance metrics.\n",
    "    :param batch_size: batch size for the data loaders\n",
    "    :type batch_size: int\n",
    "    :param num_epochs: the number of epochs to train the model\n",
    "    :type num_epochs: int\n",
    "    :param embedding_dim: the dimension of the embedding layer\n",
    "    :type embedding_dim: int\n",
    "    :param dropout_rate: the dropout rate for the model\n",
    "    :type dropout_rate: float\n",
    "    :param lr: the learning rate for the optimizer\n",
    "    :type lr: float\n",
    "    :return: a dictionary containing the trained model and the training and validation performance metrics\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    train_loader, val_loader = get_data_loaders(batch_size)\n",
    "    model = ProteinSecondaryStructureFCNwithEmbedding(embedding_dim=embedding_dim, dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Cross entropy loss function for classification problems\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam optimizer\n",
    "\n",
    "    # initialize lists to store performance metrics\n",
    "    metrics = {\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'train_precision': [],\n",
    "        'train_recall': [],\n",
    "        'train_f1': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "\n",
    "        train_losses, train_true, train_pred = [], [], []\n",
    "\n",
    "        # Training loop\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            pssms = batch['pssm'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences, pssms)\n",
    "            loss = criterion(outputs.reshape(-1, 3), labels.reshape(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the training loss\n",
    "            train_losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            train_true.extend(labels.view(-1).cpu().numpy())\n",
    "            train_pred.extend(predicted.view(-1).cpu().numpy())\n",
    "\n",
    "        # Calculate the training performance metrics\n",
    "        metrics['train_loss'].append(np.mean(train_losses))\n",
    "        metrics['train_accuracy'].append(accuracy_score(train_true, train_pred))\n",
    "        metrics['train_precision'].append(precision_score(train_true, train_pred, average='macro', zero_division=0))\n",
    "        metrics['train_recall'].append(recall_score(train_true, train_pred, average='macro', zero_division=0))\n",
    "        metrics['train_f1'].append(f1_score(train_true, train_pred, average='macro', zero_division=0))\n",
    "\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {metrics[\"train_loss\"][-1]:.4f}, Train Accuracy: {metrics[\"train_accuracy\"][-1]:.4f}, Train Precision: {metrics[\"train_precision\"][-1]:.4f}, Train Recall: {metrics[\"train_recall\"][-1]:.4f}, Train F1: {metrics[\"train_f1\"][-1]:.4f}')\n",
    "        \n",
    "        torch.cuda.empty_cache()  # clean up CUDA memory\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_losses, val_true, val_pred = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                pssms = batch['pssm'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(sequences, pssms)\n",
    "                loss = criterion(outputs.reshape(-1, 3), labels.reshape(-1))\n",
    "\n",
    "                # Track the validation loss\n",
    "                val_losses.append(loss.item())\n",
    "                _, predicted = torch.max(outputs, 2)\n",
    "                val_true.extend(labels.view(-1).cpu().numpy())\n",
    "                val_pred.extend(predicted.view(-1).cpu().numpy())\n",
    "\n",
    "        # Calculate the validation performance metrics\n",
    "        metrics['val_loss'].append(np.mean(val_losses))\n",
    "        metrics['val_accuracy'].append(accuracy_score(val_true, val_pred))\n",
    "        metrics['val_precision'].append(precision_score(val_true, val_pred, average='macro', zero_division=0))\n",
    "        metrics['val_recall'].append(recall_score(val_true, val_pred, average='macro', zero_division=0))\n",
    "        metrics['val_f1'].append(f1_score(val_true, val_pred, average='macro', zero_division=0))\n",
    "\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {metrics[\"val_loss\"][-1]:.4f}, Validation Accuracy: {metrics[\"val_accuracy\"][-1]:.4f}, Validation Precision: {metrics[\"val_precision\"][-1]:.4f}, Validation Recall: {metrics[\"val_recall\"][-1]:.4f}, Validation F1: {metrics[\"val_f1\"][-1]:.4f}')\n",
    "        \n",
    "        # torch.cuda.empty_cache()  # clean up CUDA memory\n",
    "        \n",
    "    return {'model': model, 'metrics': metrics}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.587648Z",
     "start_time": "2024-03-18T12:29:32.573510Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Train the model and evaluate the performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.603702Z",
     "start_time": "2024-03-18T12:29:32.590647Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.619709Z",
     "start_time": "2024-03-18T12:29:32.604705Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.635703Z",
     "start_time": "2024-03-18T12:29:32.620704Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Ensure you have defined and instantiated your DataLoader here\n",
    "# train_rsults = train_model(batch_size=32, num_epochs=10, embedding_dim=15, dropout_rate=0.234673, lr=0.000143)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-16T17:34:38.637421Z",
     "iopub.execute_input": "2024-03-16T17:34:38.637937Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.650857Z",
     "start_time": "2024-03-18T12:29:32.636704Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# display(train_rsults['model'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.666370Z",
     "start_time": "2024-03-18T12:29:32.651849Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Save the trained model\n",
    "# torch.save(train_rsults['model'].state_dict(), 'prediction_model_default_para.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.681457Z",
     "start_time": "2024-03-18T12:29:32.667370Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_loss_accuracy_history(metrics):\n",
    "    \"\"\"\n",
    "    绘制训练和验证损失及准确率的趋势图。\n",
    "    \"\"\"\n",
    "    train_loss_history = metrics['train_loss']\n",
    "    val_loss_history = metrics['val_loss']\n",
    "    train_accuracy_history = metrics['train_accuracy']\n",
    "    val_accuracy_history = metrics['val_accuracy']\n",
    "    \n",
    "    epochs = range(1, len(train_loss_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss_history, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss_history, label='Validation Loss')\n",
    "    plt.title('Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracy_history, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracy_history, label='Validation Accuracy')\n",
    "    plt.title('Accuracy History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.697454Z",
     "start_time": "2024-03-18T12:29:32.682456Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# display(train_rsults['metrics'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.712616Z",
     "start_time": "2024-03-18T12:29:32.698456Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "# plot_loss_accuracy_history(train_rsults['metrics'])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.728618Z",
     "start_time": "2024-03-18T12:29:32.714614Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Hyperparameter optimization with Ax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_evaluate(parameterization):\n",
    "    \"\"\"\n",
    "    It is used to train and evaluate the protein secondary structure prediction model with the given hyperparameters.\n",
    "    :param parameterization: A dictionary containing the hyperparameters to use for training and evaluation\n",
    "    :type parameterization: dict\n",
    "    :return: A dictionary containing the validation performance metrics\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    # extract the hyperparameters from the parameterization\n",
    "    learning_rate = parameterization[\"lr\"]\n",
    "    dropout_rate = parameterization[\"dropout_rate\"]\n",
    "    batch_size = parameterization[\"batch_size\"]\n",
    "    embedding_dim = parameterization[\"embedding_dim\"]\n",
    "\n",
    "    # use the extracted hyperparameters to train and evaluate the model\n",
    "    metrics = \\\n",
    "        train_model(batch_size=batch_size, embedding_dim=embedding_dim, dropout_rate=dropout_rate, lr=learning_rate)[\n",
    "            'metrics']\n",
    "\n",
    "    val_loss = metrics['val_loss'][-1]\n",
    "    val_accuracy = metrics['val_accuracy'][-1]\n",
    "    val_precision = metrics['val_precision'][-1]\n",
    "    val_recall = metrics['val_recall'][-1]\n",
    "    val_f1 = metrics['val_f1'][-1]\n",
    "\n",
    "    return {\"val_loss\": (val_loss, 0.0), \"val_accuracy\": (val_accuracy, 0.0), \"val_precision\": (val_precision, 0.0),\n",
    "            \"val_recall\": (val_recall, 0.0), \"val_f1\": (val_f1, 0.0)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.743623Z",
     "start_time": "2024-03-18T12:29:32.729616Z"
    }
   },
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ax_client = AxClient()\n",
    "# ax_client.create_experiment(\n",
    "#     name=\"protein_structure_prediction_experiment\",\n",
    "#     parameters=[\n",
    "#         {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [5e-4, 3e-3], \"log_scale\": True},\n",
    "#         {\"name\": \"dropout_rate\", \"type\": \"range\", \"bounds\": [0.15, 0.2]},\n",
    "#         {\"name\": \"batch_size\", \"type\": \"choice\", \"values\": [32, 64]},\n",
    "#         {\"name\": \"embedding_dim\", \"type\": \"choice\", \"values\": [4, 5, 6, 7]},\n",
    "#     ],\n",
    "#     objectives={\"val_loss\": ObjectiveProperties(minimize=True)},\n",
    "#     tracking_metric_names=[\"val_accuracy\", \"val_precision\", \"val_recall\", \"val_f1\"],\n",
    "# )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.759139Z",
     "start_time": "2024-03-18T12:29:32.744623Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# times_ax = 10\n",
    "# for i in range(times_ax):\n",
    "#     print(f\"Running optimization iteration {i+1}/{times_ax}...\")\n",
    "#     parameters, trial_index = ax_client.get_next_trial()\n",
    "#     ax_client.complete_trial(trial_index=trial_index, raw_data=train_evaluate(parameters)) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.774143Z",
     "start_time": "2024-03-18T12:29:32.760137Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# best_parameters_with_eval_value = ax_client.get_best_parameters()\n",
    "# display(best_parameters_with_eval_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.790137Z",
     "start_time": "2024-03-18T12:29:32.775138Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# best_parameters = best_parameters_with_eval_value[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.806138Z",
     "start_time": "2024-03-18T12:29:32.791138Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show the best parameters after ax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# best_parameters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.822135Z",
     "start_time": "2024-03-18T12:29:32.807136Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "After 80+ times of optimization, the best parameters are:\n",
    "{'lr': 0.0010683100009095904,\n",
    " 'dropout_rate': 0.1748906408059711,\n",
    " 'batch_size': 64,\n",
    " 'embedding_dim': 7}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# results_df = ax_client.get_trials_data_frame()\n",
    "# results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.838139Z",
     "start_time": "2024-03-18T12:29:32.823137Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# results_df.to_csv('10_trials_2_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.853413Z",
     "start_time": "2024-03-18T12:29:32.839139Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting to aid in parameterisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# render(ax_client.get_contour_plot(param_x=\"lr\", param_y=\"dropout_rate\", metric_name=\"val_accuracy\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.868931Z",
     "start_time": "2024-03-18T12:29:32.855412Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# render(ax_client.get_contour_plot(param_x=\"dropout_rate\", param_y=\"embedding_dim\", metric_name=\"val_accuracy\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.884927Z",
     "start_time": "2024-03-18T12:29:32.869925Z"
    }
   },
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# render(ax_client.get_contour_plot(param_x=\"lr\", param_y=\"embedding_dim\", metric_name=\"val_accuracy\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.900924Z",
     "start_time": "2024-03-18T12:29:32.885926Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# render(ax_client.get_optimization_trace())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.916032Z",
     "start_time": "2024-03-18T12:29:32.901927Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from ax.plot.slice import plot_slice\n",
    "# render(plot_slice(\n",
    "#     model=ax_client.generation_strategy.model,\n",
    "#     param_name='lr',\n",
    "#     metric_name='val_accuracy'\n",
    "# ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.932030Z",
     "start_time": "2024-03-18T12:29:32.917030Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# render(plot_slice(\n",
    "#     model=ax_client.generation_strategy.model,\n",
    "#     param_name='embedding_dim',\n",
    "#     metric_name='val_accuracy'\n",
    "# ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.947211Z",
     "start_time": "2024-03-18T12:29:32.933030Z"
    }
   },
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# render(plot_slice(\n",
    "#     model=ax_client.generation_strategy.model,\n",
    "#     param_name='dropout_rate',\n",
    "#     metric_name='val_accuracy'\n",
    "# ))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.962719Z",
     "start_time": "2024-03-18T12:29:32.949210Z"
    }
   },
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Train the model with the best hyperparameters and make predictions of the test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_parameters = {'lr': 0.0010683100009095904,\n",
    "                   'dropout_rate': 0.1748906408059711,\n",
    "                   'batch_size': 64,\n",
    "                   'embedding_dim': 7}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.977821Z",
     "start_time": "2024-03-18T12:29:32.964722Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_train(best_parameters):\n",
    "\n",
    "    batch_size = best_parameters['batch_size']\n",
    "    embedding_dim = best_parameters['embedding_dim']\n",
    "    dropout_rate = best_parameters['dropout_rate']\n",
    "    lr = best_parameters['lr']\n",
    "    num_epochs = 100\n",
    "\n",
    "    best_results = train_model(batch_size=batch_size, num_epochs=num_epochs, embedding_dim=embedding_dim, dropout_rate=dropout_rate, lr=lr)\n",
    "    \n",
    "    # save the trained model\n",
    "    filename = f\"{batch_size}_{num_epochs}_{embedding_dim}_{dropout_rate}_{lr}\"\n",
    "    torch.save(best_results['model'].state_dict(), f'{filename}.pth')\n",
    "    \n",
    "    return best_results, filename    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:29:32.992817Z",
     "start_time": "2024-03-18T12:29:32.978818Z"
    }
   },
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100:  12%|█▏        | 11/90 [00:04<00:19,  4.11it/s]"
     ]
    }
   ],
   "source": [
    "best_results, filename = test_train(best_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-18T12:29:32.993818Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict_and_save_test_set_modified(model, batch_size, filename):\n",
    "\n",
    "    seqs_test_path = 'kaggle/input/deep-learning-for-msc-202324/seqs_test.csv'\n",
    "    pssm_test_path = 'kaggle/input/deep-learning-for-msc-202324/test/'\n",
    "    output_csv_path = f'kaggle/output/{filename}.csv'\n",
    "    \n",
    "    struct_to_ix = {'H': 0, 'E': 1, 'C': 2}\n",
    "    ix_to_struct = {v: k for k, v in struct_to_ix.items()}\n",
    "\n",
    "    # initialize the test dataset and data loader\n",
    "    dataset_test = ProteinDataset(seqs_test_path, pssm_test_path)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False,  collate_fn=protein_collate_fn, pin_memory=True)\n",
    "\n",
    "    submission = [['ID', 'STRUCTURE']]\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch in enumerate(dataloader_test):\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            pssms = batch['pssm'].to(device)\n",
    "            pdb_ids = batch['pdb_id']\n",
    "            real_lens = batch['real_len']\n",
    "            \n",
    "            print(f\"Batch {batch_index + 1}/{len(dataloader_test)}\")\n",
    "\n",
    "            outputs = model(sequences, pssms)\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "\n",
    "            # split the batch predictions into individual sequences\n",
    "            for i, pdb_id in enumerate(pdb_ids):\n",
    "                real_len = real_lens[i].item()\n",
    "                # print(f\"{pdb_id} with {real_len} residues\")\n",
    "                \n",
    "                for j in range(real_len):\n",
    "                    residue_prediction = predicted[i][j].cpu().item()\n",
    "                    submission.append([f\"{pdb_id}_{j+1}\", ix_to_struct[residue_prediction]])\n",
    "                    # print(f\"{pdb_id}_{j+1} with {ix_to_struct[residue_prediction]}\")\n",
    "                \n",
    "\n",
    "    # save the predictions to a CSV file\n",
    "    with open(output_csv_path, 'w') as f:\n",
    "        for line in submission:\n",
    "            f.write(','.join(line) + '\\n')\n",
    "            \n",
    "    print(f\"Total predictions made: {len(submission) - 1}\")\n",
    "    print(f'file saved to {output_csv_path}')\n",
    "\n",
    "    return model, dataloader_test"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "filename"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_model, dataloader_test = predict_and_save_test_set_modified(best_results['model'], best_parameters['batch_size'], filename)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_loss_accuracy_history(best_results['metrics'])\n",
    "# render(ax_client.get_contour_plot(param_x=\"lr\", param_y=\"momentum\", metric_name=\"accuracy\"))\n",
    "# render(ax_client.get_optimization_trace())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Explain the model predictions with Captum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def visualize_attributions(attributions_seq, attributions_pssm, amino_acids=None):\n",
    "    \"\"\"\n",
    "    可视化氨基酸序列和PSSM特征的归因分数。\n",
    "    \n",
    "    参数:\n",
    "    - attributions_seq: 氨基酸序列的归因分数，形状为(batch_size, seq_len)。\n",
    "    - attributions_pssm: PSSM特征的归因分数，形状为(batch_size, seq_len, num_features)。\n",
    "    - amino_acids: 可选，氨基酸序列的列表，用于x轴标签。\n",
    "    \"\"\"\n",
    "    # 假设我们只可视化第一个样本\n",
    "    sample_attributions_seq = attributions_seq[0]\n",
    "    sample_attributions_pssm = attributions_pssm[0]\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "    # 绘制氨基酸序列的归因分数条形图\n",
    "    ax[0].bar(range(len(sample_attributions_seq)), sample_attributions_seq)\n",
    "    ax[0].set_title('氨基酸序列的归因分数')\n",
    "    ax[0].set_xlabel('氨基酸位置')\n",
    "    ax[0].set_ylabel('归因分数')\n",
    "    if amino_acids:\n",
    "        ax[0].set_xticks(range(len(amino_acids)))\n",
    "        ax[0].set_xticklabels(amino_acids, rotation=45, ha=\"right\")\n",
    "\n",
    "    # 绘制PSSM特征的归因分数热图\n",
    "    cax = ax[1].matshow(sample_attributions_pssm, aspect='auto', cmap='viridis')\n",
    "    fig.colorbar(cax, ax=ax[1])\n",
    "    ax[1].set_title('PSSM特征的归因分数')\n",
    "    ax[1].set_xlabel('特征')\n",
    "    ax[1].set_ylabel('氨基酸位置')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def attributions(model, dataloader_test):\n",
    "    # integrated_gradients = IntegratedGradients(model)\n",
    "    # \n",
    "    # sample = next(iter(dataloader_test))\n",
    "    # sequences = sample['sequence'].to(device)\n",
    "    # pssms = sample['pssm'].to(device)\n",
    "    # \n",
    "    # attributions = integrated_gradients.attribute((sequences, pssms), target=0)\n",
    "    # attributions = attributions.cpu().detach().numpy()\n",
    "    # \n",
    "    # \n",
    "    # # attributions_seq = integrated_gradients.attribute(sequences, target=0)  # target是所求梯度的类索引\n",
    "    # # attributions_pssm = integrated_gradients.attribute(pssms, target=0)\n",
    "    # # \n",
    "    # # attributions_seq = attributions_seq.cpu().detach().numpy()\n",
    "    # # attributions_pssm = attributions_pssm.cpu().detach().numpy()\n",
    "    # \n",
    "    # return attributions\n",
    "    \n",
    "    # 归因计算逻辑\n",
    "    # 注意：这里假设你想要对测试集中的第一个样本进行归因分析\n",
    "    sample = next(iter(dataloader_test))\n",
    "    sequences = sample['sequence'].to(device)\n",
    "    pssms = sample['pssm'].to(device)\n",
    "\n",
    "    # 确保使用模型的评估模式\n",
    "    model.eval()\n",
    "\n",
    "    # 初始化IntegratedGradients实例\n",
    "    integrated_gradients = IntegratedGradients(model)\n",
    "\n",
    "    # 计算归因\n",
    "    # 重要：确保传递一个包含所有输入的元组，并且每个输入都是适当的数据类型\n",
    "    attributions_seq, attributions_pssm = integrated_gradients.attribute(inputs=(sequences, pssms), target=0, additional_forward_args=None)\n",
    "    attributions_seq = attributions_seq.cpu().detach().numpy()\n",
    "    attributions_pssm = attributions_pssm.cpu().detach().numpy()\n",
    "\n",
    "    # 这里，你可以根据需要进一步处理或保存归因结果\n",
    "    ...\n",
    "\n",
    "    return attributions_seq, attributions_pssm"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "attributions_seq, attributions_pssm = attributions(test_model, dataloader_test)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "visualize_attributions(attributions_seq, attributions_pssm)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  }
 ]
}
